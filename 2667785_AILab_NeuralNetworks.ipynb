{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c610085-c06b-4b0e-b39b-36a9da68bdb2",
   "metadata": {},
   "source": [
    "<a href src=\"https://github.com/bucheeri5/saradoesai.github.io\">Heres a link to my repositry</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1cdf46",
   "metadata": {},
   "source": [
    "# Generating Text with Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c62b6d-be98-4b01-8bac-a5c6a043b0c6",
   "metadata": {},
   "source": [
    "This code employs neural networks to generate text based on Shakespeare literature. It uses TensorFlow, which is a Python library for building neural networks. This model shows how the humanities can be engaged within LLMs, beyond general generative AI. Yet, building specialised data sets and categorising them for machine learning is challenging because it requires significant resources. Notably, this project required significant computing energy and time to run. This means it needs computers with higher capacity. This project reduced the training data due to the capacity of my personal computer, and this notably affected the data as some of the outputs were incoherent and others were gibberish. The implications of these points are important because, at the current stage of humanities and low funding, proceeding with highly specialised projects like this on a much larger scale might require more resources, which could be ambitious within the humanities. Neural networks are also slightly harder to interpret because they require advanced code, so this project added comments to allow a wider audience to engage with the process. However, it also points out how humanities students should be equipped with digital literacy skills in order to integrate with modern-day technologies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70325e9",
   "metadata": {},
   "source": [
    "# Getting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b5298b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "shakespeare_url = \"https://homl.info/shakespeare\"  # shortcut URL\n",
    "filepath = tf.keras.utils.get_file(\"shakespeare.txt\", shakespeare_url) \n",
    "with open(filepath) as f: \n",
    "    shakespeare_text = f.read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac39fd44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n"
     ]
    }
   ],
   "source": [
    "print(shakespeare_text[:80]) # This will print the first 80 characters of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de55b2aa",
   "metadata": {},
   "source": [
    "# Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb3cf03-f007-4c87-a77d-14dc4c812d5e",
   "metadata": {},
   "source": [
    "The code converts Shakespeare's dataset into a format understandable by a computer, enabling computational analysis. It helps in tasks like understanding patterns, sentiment analysis, or building models for various natural language processing applications. This section of code snippet sets the length of sequences to 100 and initializes a random seed for reproducibility using TensorFlow. Following that it creates three datasets—training, validation, and test sets—using the to_dataset function. The training set is derived from the first 125,000 elements of the encoded data, with sequences of the specified length, and is shuffled for randomness. Imagine the text as one long story, and this code is organizing it into three parts: a training section to teach the program, a validation section to check its learning, and a test section to see how well it can apply what it learned. The length of each piece of the story that the program sees at once is set to 100 characters, like a small snippet.  The validation set comprises sequences from 125,000 to 132,500 of the encoded data. Lastly, the test set is formed from elements starting from the 132,500th position in the encoded data, all with sequences of the specified length. These datasets are likely intended for training and evaluating a machine learning model on the processed Shakespearean text data. The goal is to train a computer model to understand and generate text in a way that resembles Shakespeare's writing style. Preparing the data is the most time consuming part in terms of the process but the most important! This is what our neural network learns from, so we need to figure out what we are feeding it.\n",
    "\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05fbfd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec_layer = tf.keras.layers.TextVectorization(split=\"character\",\n",
    "                                                   standardize=\"lower\") #This line creates a layer that converts text into numerical vectors. It takes two arguments: split and standardize. split tells the layer to split the text into individual characters, and standardize tells it to convert all letters to lowercase.\n",
    "text_vec_layer.adapt([shakespeare_text]) # This line adapts the layer to the text data. It's like telling the layer, \"Hey, I'm going to give you some text to work with. Get ready!\"\n",
    "encoded = text_vec_layer([shakespeare_text])[0] # This line applies the layer to the text data and gets the output. The [0] at the end tells it to return the first element of the output, as python code starts counting from 0 instead of 1, which represents the encoded text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "573e8c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[21  7 10 ... 22 28 12]], shape=(1, 1115394), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(text_vec_layer([shakespeare_text])) #this code displays the numerical representation of the text after the processing performed by the text vectorization layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bcd3ce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded -= 2  # drop tokens 0 (pad) and 1 (unknown), which we will not use\n",
    "n_tokens = text_vec_layer.vocabulary_size() - 2  # number of distinct chars = 39\n",
    "dataset_size = len(encoded)  # total number of chars = 1,115,394"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4c79b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 1115394\n"
     ]
    }
   ],
   "source": [
    "print(n_tokens, dataset_size) # Print the number of distinct characters and dataset size which is the number of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8571a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    ds = ds.window(length + 1, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(100_000, seed=seed)\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba80acc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 100\n",
    "tf.random.set_seed(42) # Set random seed for reproducibility\n",
    "\n",
    "train_set = to_dataset(encoded[:125_000:], length=length, shuffle=True,\n",
    "                       seed=42)\n",
    "valid_set = to_dataset(encoded[125_000:132_500], length=length)\n",
    "test_set = to_dataset(encoded[132_500:], length=length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d65051",
   "metadata": {},
   "source": [
    "# Building and Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f5d447-af62-42a4-8aff-9942dcbe25b4",
   "metadata": {},
   "source": [
    "In this section, the goal is to make the model learn from the organised Shakespearean text selected and snipped in the previous step. The code establishes a random seed to make sure that the results are reproducible, meaning if I were to run the programme multiple times, I'd get the same outcomes. The programme is structured in layers. First, it sets up a layer to convert the individual characters into numerical representations. Then, it uses a type of layer called GRU to understand patterns in the data, and finally, it has a layer that predicts the next character in the sequence. The model is trained using a specific loss function and an optimisation algorithm. The loss function guides the model during training by penalizing it when the predicted probabilities diverge and stray away from the characters observed in the training data that we specified earlier on. Minimising this loss is the objective of the training process, as it leads the model to make more accurate predictions over time. The number of epochs is specified by the epochs=10 parameter in the model.fit function. In this case, the model will undergo training for ten epochs, meaning it will iterate through the entire train_set dataset ten times. Adjusting the number of epochs is a common hyperparameter tuning step in machine learning, and the optimal value often depends on the specific characteristics of the dataset and the complexity and size of the model. Increasing the data size and epochs will optimise the results, but that also requires more computing power and processing time. The choice is to be made by the developers, depending on how accurate the outcomes we want them to be! If you are really keen on Shakespear and are offended when people misquote him, then you might want to increase the data size and epochs. I am not, so I reduced the data. In this code, there's a checkpoint set up to save the best version of the model during training, and the training history is stored for later analysis. This entire process is aimed at creating a model that can generate text in a way that mimics Shakespeare's writing style. This is a very time-consuming process to run, but it is basically waiting for the computer to marinate the data and analyse it based on what we told it to. Here, we tell it what we have, and we sit back and relax till it loads.\n",
    "\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f19e4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "   3904/Unknown - 632s 149ms/step - loss: 1.6559 - accuracy: 0.5064"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_shakespeare_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_shakespeare_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3904/3904 [==============================] - 669s 159ms/step - loss: 1.6559 - accuracy: 0.5064 - val_loss: 1.5986 - val_accuracy: 0.5222\n",
      "Epoch 2/10\n",
      "3904/3904 [==============================] - 542s 130ms/step - loss: 1.1708 - accuracy: 0.6402 - val_loss: 1.7916 - val_accuracy: 0.5007\n",
      "Epoch 3/10\n",
      "3904/3904 [==============================] - 636s 154ms/step - loss: 1.0212 - accuracy: 0.6869 - val_loss: 1.9403 - val_accuracy: 0.4944\n",
      "Epoch 5/10\n",
      "3904/3904 [==============================] - 611s 148ms/step - loss: 1.0022 - accuracy: 0.6927 - val_loss: 1.9829 - val_accuracy: 0.4892\n",
      "Epoch 6/10\n",
      "3904/3904 [==============================] - 737s 178ms/step - loss: 0.9900 - accuracy: 0.6965 - val_loss: 1.9963 - val_accuracy: 0.4907\n",
      "Epoch 7/10\n",
      "3904/3904 [==============================] - 661s 160ms/step - loss: 0.9809 - accuracy: 0.6993 - val_loss: 2.0083 - val_accuracy: 0.4892\n",
      "Epoch 8/10\n",
      "3904/3904 [==============================] - 638s 154ms/step - loss: 0.9735 - accuracy: 0.7017 - val_loss: 2.0282 - val_accuracy: 0.4886\n",
      "Epoch 9/10\n",
      "3904/3904 [==============================] - 600s 143ms/step - loss: 0.9677 - accuracy: 0.7034 - val_loss: 2.0416 - val_accuracy: 0.4894\n",
      "Epoch 10/10\n",
      "3904/3904 [==============================] - 607s 147ms/step - loss: 0.9626 - accuracy: 0.7049 - val_loss: 2.0561 - val_accuracy: 0.4881\n"
     ]
    }
   ],
   "source": [
    "# Set a random seed to ensure reproducibility of results\n",
    "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
    "    tf.keras.layers.GRU(128, return_sequences=True),\n",
    "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
    "])\n",
    "# Compile the model with specific settings\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "# Model checkpoint callback\n",
    "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"my_shakespeare_model\", monitor=\"val_accuracy\", save_best_only=True)\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=10,\n",
    "                    callbacks=[model_ckpt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0216893d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m shakespeare_model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[0;32m      2\u001b[0m     text_vec_layer,\n\u001b[0;32m      3\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mLambda(\u001b[38;5;28;01mlambda\u001b[39;00m X: X \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m),  \u001b[38;5;66;03m# no <PAD> or <UNK> tokens\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mmodel\u001b[49m\n\u001b[0;32m      5\u001b[0m ])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "shakespeare_model = tf.keras.Sequential([\n",
    "    text_vec_layer,\n",
    "    tf.keras.layers.Lambda(lambda X: X - 2),  # no <PAD> or <UNK> tokens\n",
    "    model\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8927fee3",
   "metadata": {},
   "source": [
    "# Generating Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d5a728-b6c6-46f8-9c6f-37f91685571f",
   "metadata": {},
   "source": [
    "Arguably, this is the most fun part of this process! This is because we get to test whether the text aligns with Shakespear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0581742",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shakespeare_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_proba \u001b[38;5;241m=\u001b[39m \u001b[43mshakespeare_model\u001b[49m\u001b[38;5;241m.\u001b[39mpredict([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo be or not to b\u001b[39m\u001b[38;5;124m\"\u001b[39m])[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      2\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39margmax(y_proba)  \u001b[38;5;66;03m# choose the most probable character ID\u001b[39;00m\n\u001b[0;32m      3\u001b[0m text_vec_layer\u001b[38;5;241m.\u001b[39mget_vocabulary()[y_pred \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'shakespeare_model' is not defined"
     ]
    }
   ],
   "source": [
    "y_proba = shakespeare_model.predict([\"To be or not to b\"])[0, -1]\n",
    "y_pred = tf.argmax(y_proba)  # choose the most probable character ID\n",
    "text_vec_layer.get_vocabulary()[y_pred + 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b80ac03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 8), dtype=int64, numpy=array([[0, 1, 0, 2, 1, 0, 0, 1]], dtype=int64)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probas = tf.math.log([[0.5, 0.4, 0.1]])  # probas = 50%, 40%, and 10%\n",
    "tf.random.set_seed(42)\n",
    "tf.random.categorical(log_probas, num_samples=8)  # draw 8 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09519ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    y_proba = shakespeare_model.predict([text])[0, -1:]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0, 0]\n",
    "    return text_vec_layer.get_vocabulary()[char_id + 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b91b91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9085f1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9632ff6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shakespeare_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mextend_text\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTo be or not to be\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m, in \u001b[0;36mextend_text\u001b[1;34m(text, n_chars, temperature)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextend_text\u001b[39m(text, n_chars\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_chars):\n\u001b[1;32m----> 3\u001b[0m         text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mnext_char\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m, in \u001b[0;36mnext_char\u001b[1;34m(text, temperature)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnext_char\u001b[39m(text, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     y_proba \u001b[38;5;241m=\u001b[39m \u001b[43mshakespeare_model\u001b[49m\u001b[38;5;241m.\u001b[39mpredict([text])[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m      3\u001b[0m     rescaled_logits \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mlog(y_proba) \u001b[38;5;241m/\u001b[39m temperature\n\u001b[0;32m      4\u001b[0m     char_id \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mcategorical(rescaled_logits, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'shakespeare_model' is not defined"
     ]
    }
   ],
   "source": [
    "print(extend_text(\"To be or not to be\", temperature=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16befa5b-90ed-4cbb-bf9e-dedc09ee8ed0",
   "metadata": {},
   "source": [
    "The statement \"To be or not to be their people\" is not a very coherent or clear statement, as it grammatically incomplete. It is difficult to understand whhat it means by \"their people\". Further, \"his neck\" might be due to executions during Shakespear's time, but again, it is not very coherent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "77d1a893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "To be or not to be grave otheraked you.\n",
      "\n",
      "volumnia:\n",
      "i, good me wine, \n"
     ]
    }
   ],
   "source": [
    "print(extend_text(\"To be or not to be\", temperature=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf3f616-a554-4ede-9b92-9e7b52998616",
   "metadata": {},
   "source": [
    "Again, this shows some elements from Shakespear's literature specificially Volumnia who is a character in the play Coriolanus. However, it is not comprehensible.\n",
    "\n",
    "Here's a breakdown of HuggingChat interpreted this code:https://huggingface.co/chat\n",
    "\n",
    "\n",
    "\"To be or not to be grave\" - This plays off the original phrase \"To be or not to be,\"  By adding \"grave\" to the end, it turns the phrase into a pun about being serious (grave) or not being serious (not grave). \"otheraked\" - This is a play on words combining \"other\" and \"baked.\" It's a made-up word that suggests something that is baked, but also somehow different or unusual.\n",
    "\"I, good me wine\" - This line is a play on the phrase \"I am good wine,\" which is a boastful claim to being of high quality or vintage. By adding \"me\" instead of \"am,\" it becomes a bit more casual and conversational, like a friendly taunt.\n",
    "So, putting it all together, the joke is essentially saying, \"To be or not to be serious, that is the question. But let's be real, I'm already good wine, so bring on the laughs!\" It's a lighthearted way to poke fun at the idea of taking oneself too seriously, and invites the listener to join in on the humor. Well done!\n",
    "\n",
    "However, the intrepetation to me as a human was not making sense. It might be because I am less engaged with western literature and literary techniques but I think it is still not clear. However, I believe that the word \"otheraked\" is mimicking how Shakespear was invenetive with language. He was well-known for coining new terms that were derived from existing ones, regardless if the model was really poor at this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c6803df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extend_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mextend_text\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo be or not to be\u001b[39m\u001b[38;5;124m\"\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'extend_text' is not defined"
     ]
    }
   ],
   "source": [
    "print(extend_text(\"To be or not to be\", temperature=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357f5765-73bc-41ad-8bb8-842d4cd1dd3a",
   "metadata": {},
   "source": [
    "Overall, this model is proving itself poor in the quality. One of the most reasonable explanations for this is that I reduced the size of the data to have a quicker loading time. When I compared outcome with my peer support group, they did have more coherent data in comparison to mine, which explains my outcomes. However, it is admirable that the dataset is only based on Shakespear and was able to produce semi-impressive results with my peers who ran all the data available. However, as mentioned in the beginning this raises issues about how viable it is within the humanities. Perhaps cultural items that are generally percieved as more prestigious and favorable by other people will have more datasets, but works from lesser known artists might not be pursued for such datasets. It would be interesting to see the application of this within exhibitions, such as robots that pretend to be certain figures from history that have been mimicked through generative AI. Once again, this would require extensive resources!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
